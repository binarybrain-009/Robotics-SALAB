{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip install numpy pandas matplotlib tensorflow gym\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qTtEeSLWk7vV",
        "outputId": "f76d0da5-de8b-4165-bd45-f7e4736ee11f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (1.26.4)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (2.2.2)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.10/dist-packages (3.8.0)\n",
            "Requirement already satisfied: tensorflow in /usr/local/lib/python3.10/dist-packages (2.17.1)\n",
            "Requirement already satisfied: gym in /usr/local/lib/python3.10/dist-packages (0.25.2)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas) (2024.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.10/dist-packages (from pandas) (2024.2)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (1.3.1)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (4.55.3)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (1.4.7)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (24.2)\n",
            "Requirement already satisfied: pillow>=6.2.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (11.0.0)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (3.2.0)\n",
            "Requirement already satisfied: absl-py>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.4.0)\n",
            "Requirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.6.3)\n",
            "Requirement already satisfied: flatbuffers>=24.3.25 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (24.3.25)\n",
            "Requirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (0.6.0)\n",
            "Requirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (0.2.0)\n",
            "Requirement already satisfied: h5py>=3.10.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (3.12.1)\n",
            "Requirement already satisfied: libclang>=13.0.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (18.1.1)\n",
            "Requirement already satisfied: ml-dtypes<0.5.0,>=0.3.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (0.4.1)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (3.4.0)\n",
            "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.20.3 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (4.25.5)\n",
            "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (2.32.3)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from tensorflow) (75.1.0)\n",
            "Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.17.0)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (2.5.0)\n",
            "Requirement already satisfied: typing-extensions>=3.6.6 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (4.12.2)\n",
            "Requirement already satisfied: wrapt>=1.11.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.17.0)\n",
            "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.68.1)\n",
            "Requirement already satisfied: tensorboard<2.18,>=2.17 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (2.17.1)\n",
            "Requirement already satisfied: keras>=3.2.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (3.5.0)\n",
            "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (0.37.1)\n",
            "Requirement already satisfied: cloudpickle>=1.2.0 in /usr/local/lib/python3.10/dist-packages (from gym) (3.1.0)\n",
            "Requirement already satisfied: gym_notices>=0.0.4 in /usr/local/lib/python3.10/dist-packages (from gym) (0.0.8)\n",
            "Requirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.10/dist-packages (from astunparse>=1.6.0->tensorflow) (0.45.1)\n",
            "Requirement already satisfied: rich in /usr/local/lib/python3.10/dist-packages (from keras>=3.2.0->tensorflow) (13.9.4)\n",
            "Requirement already satisfied: namex in /usr/local/lib/python3.10/dist-packages (from keras>=3.2.0->tensorflow) (0.0.8)\n",
            "Requirement already satisfied: optree in /usr/local/lib/python3.10/dist-packages (from keras>=3.2.0->tensorflow) (0.13.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorflow) (3.4.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorflow) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorflow) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorflow) (2024.12.14)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.18,>=2.17->tensorflow) (3.7)\n",
            "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.18,>=2.17->tensorflow) (0.7.2)\n",
            "Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.18,>=2.17->tensorflow) (3.1.3)\n",
            "Requirement already satisfied: MarkupSafe>=2.1.1 in /usr/local/lib/python3.10/dist-packages (from werkzeug>=1.0.1->tensorboard<2.18,>=2.17->tensorflow) (3.0.2)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.10/dist-packages (from rich->keras>=3.2.0->tensorflow) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from rich->keras>=3.2.0->tensorflow) (2.18.0)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.10/dist-packages (from markdown-it-py>=2.2.0->rich->keras>=3.2.0->tensorflow) (0.1.2)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-SJF6nTEkFJ3",
        "outputId": "1b6ba94f-362a-4c5a-82e8-cbee58f86676"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Maximum Success Rate: 1144.9999999999866\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Load the annotated actions CSV\n",
        "file_path = 'annotated_actions_new.csv'  # Replace with the actual path to the CSV\n",
        "actions_df = pd.read_csv(file_path)\n",
        "actions_df['features'] = actions_df['features'].fillna('').astype(str)\n",
        "\n",
        "# Extract the features column as a list of lists for subtasks\n",
        "subtasks = actions_df['features'].apply(lambda x: x.split(', ') if x else []).tolist()\n",
        "\n",
        "# Define success matrix and functions (unchanged)\n",
        "def aggregate_success(features, robot_type):\n",
        "    \"\"\"\n",
        "    Compute the aggregate success rate for a robot handling a subtask with multiple features.\n",
        "    \"\"\"\n",
        "    success_matrix = {\n",
        "        'careful': {'light': 0.9, 'middle': 0.7, 'heavy': 0.5},\n",
        "        'dexterous': {'light': 0.8, 'middle': 0.6, 'heavy': 0.4},\n",
        "        'heavy': {'light': 0.5, 'middle': 0.7, 'heavy': 0.9}\n",
        "    }\n",
        "    # Compute the average success rate across all features\n",
        "    return sum(success_matrix[feature][robot_type] for feature in features) / len(features) if features else 0\n",
        "\n",
        "def allocate_subtasks(subtasks, robots):\n",
        "    \"\"\"\n",
        "    Allocate subtasks to robots using dynamic programming.\n",
        "    subtasks: List of subtasks with their features (e.g., [['careful', 'dexterous'], ['heavy'], ...]).\n",
        "    robots: List of robot types (e.g., ['light', 'middle', 'heavy']).\n",
        "    \"\"\"\n",
        "    num_subtasks = len(subtasks)\n",
        "    num_robots = len(robots)\n",
        "\n",
        "    # Initialize DP table\n",
        "    dp = [[0] * num_robots for _ in range(num_subtasks)]\n",
        "\n",
        "    # Base case: Assign the first subtask to each robot\n",
        "    for j in range(num_robots):\n",
        "        dp[0][j] = aggregate_success(subtasks[0], robots[j])\n",
        "\n",
        "    # Fill DP table\n",
        "    for i in range(1, num_subtasks):\n",
        "        for j in range(num_robots):\n",
        "            # Assign subtask i to robot j and maximize the cumulative success rate\n",
        "            dp[i][j] = max(dp[i-1][k] + aggregate_success(subtasks[i], robots[j]) for k in range(num_robots))\n",
        "\n",
        "    # Extract the optimal solution\n",
        "    return max(dp[num_subtasks-1])\n",
        "\n",
        "# Assuming three robot types for now\n",
        "robots = ['light', 'middle', 'heavy']\n",
        "\n",
        "# Call the function with dynamically loaded subtasks\n",
        "result = allocate_subtasks(subtasks, robots)\n",
        "print(\"Maximum Success Rate:\", result)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import time\n",
        "\n",
        "def q_learning_allocation(tasks, robots, alpha=0.1, gamma=0.9, epsilon=0.1, episodes=10):\n",
        "    states = range(len(tasks))\n",
        "    actions = range(len(robots))\n",
        "    q_table = np.zeros((len(states), len(actions)))\n",
        "\n",
        "    start_time = time.time()  # Start timing\n",
        "    log_interval = episodes // 10  # Log progress every 10% of episodes\n",
        "\n",
        "    for episode in range(episodes):\n",
        "        for state in states:\n",
        "            action = random.choice(actions) if random.uniform(0, 1) < epsilon else np.argmax(q_table[state])\n",
        "\n",
        "            reward = aggregate_success(tasks[state], robots[action])\n",
        "            next_state = (state + 1) % len(states)\n",
        "            q_table[state, action] += alpha * (reward + gamma * np.max(q_table[next_state]) - q_table[state, action])\n",
        "\n",
        "        # Log progress\n",
        "        if (episode + 1) % log_interval == 0 or episode == episodes - 1:\n",
        "            elapsed_time = time.time() - start_time\n",
        "            avg_time_per_episode = elapsed_time / (episode + 1)\n",
        "            remaining_time = avg_time_per_episode * (episodes - episode - 1)\n",
        "            print(f\"Episode {episode + 1}/{episodes} - Elapsed Time: {elapsed_time:.2f}s - Estimated Remaining Time: {remaining_time:.2f}s\")\n",
        "\n",
        "    total_success_rate = 0\n",
        "    allocations = []\n",
        "    for state in states:\n",
        "        best_action = np.argmax(q_table[state])\n",
        "        task_success_rate = aggregate_success(tasks[state], robots[best_action])\n",
        "        total_success_rate += task_success_rate\n",
        "        allocations.append((state, robots[best_action], task_success_rate))\n",
        "\n",
        "    average_success_rate = total_success_rate / len(tasks)\n",
        "    return average_success_rate, allocations\n"
      ],
      "metadata": {
        "id": "Qq-SodP0oqbb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def dqn_allocation(tasks, robots, episodes=10, batch_size=32):\n",
        "    state_size = 1  # Simplified state representation (can be modified for more complex states)\n",
        "    action_size = len(robots)\n",
        "    model = build_dqn_model(state_size, action_size)\n",
        "\n",
        "    gamma = 0.9\n",
        "    epsilon = 0.1\n",
        "    memory = []\n",
        "\n",
        "    start_time = time.time()  # Start timing\n",
        "    log_interval = max(1, episodes // 10)  # Log progress every 10% of episodes\n",
        "\n",
        "    for episode in range(episodes):\n",
        "        for task in tasks:\n",
        "            state = np.array([[len(task)]], dtype=np.float32)  # Ensure state has shape (1, state_size)\n",
        "\n",
        "            if random.uniform(0, 1) < epsilon:\n",
        "                action = random.choice(range(action_size))\n",
        "            else:\n",
        "                action = np.argmax(model.predict(state, verbose=0))\n",
        "\n",
        "            reward = aggregate_success(task, robots[action])\n",
        "            next_state = np.array([[len(task)]], dtype=np.float32)  # Ensure next_state has the same shape\n",
        "            memory.append((state, action, reward, next_state))\n",
        "\n",
        "            if len(memory) >= batch_size:\n",
        "                minibatch = random.sample(memory, batch_size)\n",
        "                states_batch = np.array([s[0] for s, _, _, _ in minibatch], dtype=np.float32)\n",
        "                actions_batch = [a for _, a, _, _ in minibatch]\n",
        "                rewards_batch = [r for _, _, r, _ in minibatch]\n",
        "                next_states_batch = np.array([ns[0] for _, _, _, ns in minibatch], dtype=np.float32)\n",
        "\n",
        "                next_q_values = model.predict(next_states_batch, verbose=0)\n",
        "                targets = rewards_batch + gamma * np.max(next_q_values, axis=1)\n",
        "\n",
        "                target_f = model.predict(states_batch, verbose=0)\n",
        "                for i in range(batch_size):\n",
        "                    target_f[i][actions_batch[i]] = targets[i]\n",
        "\n",
        "                model.fit(states_batch, target_f, epochs=1, verbose=0, batch_size=batch_size)\n",
        "\n",
        "        # Log progress\n",
        "        if (episode + 1) % log_interval == 0 or episode == episodes - 1:\n",
        "            elapsed_time = time.time() - start_time\n",
        "            avg_time_per_episode = elapsed_time / (episode + 1)\n",
        "            remaining_time = avg_time_per_episode * (episodes - episode - 1)\n",
        "            print(f\"Episode {episode + 1}/{episodes} - Elapsed Time: {elapsed_time:.2f}s - Estimated Remaining Time: {remaining_time:.2f}s\")\n",
        "\n",
        "    total_success_rate = 0\n",
        "    allocations = []\n",
        "    for task in tasks:\n",
        "        state = np.array([[len(task)]], dtype=np.float32)\n",
        "        best_action = np.argmax(model.predict(state, verbose=0))\n",
        "        task_success_rate = aggregate_success(task, robots[best_action])\n",
        "        total_success_rate += task_success_rate\n",
        "        allocations.append((task, robots[best_action], task_success_rate))\n",
        "\n",
        "    average_success_rate = total_success_rate / len(tasks)\n",
        "    return average_success_rate, allocations\n"
      ],
      "metadata": {
        "id": "ddUR7n7Jortl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "\n",
        "# Define the DQN model\n",
        "def build_dqn_model(input_size, output_size):\n",
        "    \"\"\"\n",
        "    Build a Deep Q-Network model with the specified input and output sizes.\n",
        "\n",
        "    Args:\n",
        "        input_size (int): The number of input features.\n",
        "        output_size (int): The number of output Q-values (actions).\n",
        "\n",
        "    Returns:\n",
        "        model: A compiled Keras Sequential model.\n",
        "    \"\"\"\n",
        "    model = Sequential([\n",
        "        tf.keras.layers.Input(shape=(input_size,)),  # Input layer\n",
        "        Dense(40, activation='relu'),                # First hidden layer with 40 nodes\n",
        "        Dense(40, activation='relu'),                # Second hidden layer with 40 nodes\n",
        "        Dense(output_size, activation='linear')      # Output layer generating Q-values\n",
        "    ])\n",
        "    # Compile the model\n",
        "    model.compile(optimizer=Adam(learning_rate=0.001), loss='mse')\n",
        "    return model\n",
        "\n",
        "# Example usage\n",
        "input_size = 10  # Example input size (number of features)\n",
        "output_size = 5  # Example output size (number of actions)\n",
        "model = build_dqn_model(input_size, output_size)\n",
        "\n",
        "# Display the model summary\n",
        "model.summary()\n"
      ],
      "metadata": {
        "id": "pmSG_Omuozvl",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 232
        },
        "outputId": "2d911825-2797-43d4-d960-7f2e837d8e67"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1mModel: \"sequential\"\u001b[0m\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential\"</span>\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━┓\n",
              "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                        \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape               \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m        Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━┩\n",
              "│ dense (\u001b[38;5;33mDense\u001b[0m)                        │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m40\u001b[0m)                  │             \u001b[38;5;34m440\u001b[0m │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ dense_1 (\u001b[38;5;33mDense\u001b[0m)                      │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m40\u001b[0m)                  │           \u001b[38;5;34m1,640\u001b[0m │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ dense_2 (\u001b[38;5;33mDense\u001b[0m)                      │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m5\u001b[0m)                   │             \u001b[38;5;34m205\u001b[0m │\n",
              "└──────────────────────────────────────┴─────────────────────────────┴─────────────────┘\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━┓\n",
              "┃<span style=\"font-weight: bold\"> Layer (type)                         </span>┃<span style=\"font-weight: bold\"> Output Shape                </span>┃<span style=\"font-weight: bold\">         Param # </span>┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━┩\n",
              "│ dense (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                        │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">40</span>)                  │             <span style=\"color: #00af00; text-decoration-color: #00af00\">440</span> │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ dense_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                      │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">40</span>)                  │           <span style=\"color: #00af00; text-decoration-color: #00af00\">1,640</span> │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ dense_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                      │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">5</span>)                   │             <span style=\"color: #00af00; text-decoration-color: #00af00\">205</span> │\n",
              "└──────────────────────────────────────┴─────────────────────────────┴─────────────────┘\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m2,285\u001b[0m (8.93 KB)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">2,285</span> (8.93 KB)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m2,285\u001b[0m (8.93 KB)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">2,285</span> (8.93 KB)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "import random\n",
        "import gym\n",
        "\n",
        "# Load the annotated actions CSV\n",
        "file_path = 'annotated_actions_new.csv'  # Replace with the actual path to the CSV\n",
        "actions_df = pd.read_csv(file_path)\n",
        "actions_df['features'] = actions_df['features'].fillna('').astype(str)\n",
        "\n",
        "# Extract the features column as a list of lists for subtasks\n",
        "subtasks = actions_df['features'].apply(lambda x: x.split(', ') if x else []).tolist()\n",
        "\n",
        "# Define success matrix\n",
        "success_matrix = {\n",
        "    'careful': {'light': 0.9, 'middle': 0.7, 'heavy': 0.5},\n",
        "    'dexterous': {'light': 0.8, 'middle': 0.6, 'heavy': 0.4},\n",
        "    'heavy': {'light': 0.5, 'middle': 0.7, 'heavy': 0.9}\n",
        "}\n",
        "\n",
        "robots = ['light', 'middle', 'heavy']\n",
        "\n",
        "def aggregate_success(features, robot_type):\n",
        "    \"\"\"\n",
        "    Compute the aggregate success rate for a robot handling a subtask with multiple features.\n",
        "    \"\"\"\n",
        "    return sum(success_matrix[feature][robot_type] for feature in features) / len(features) if features else 0\n",
        "\n",
        "# ------------------ Q-Learning ------------------\n",
        "\n",
        "# def q_learning_allocation(tasks, robots, alpha=0.1, gamma=0.9, epsilon=0.1, episodes=1000):\n",
        "#     states = range(len(tasks))\n",
        "#     actions = range(len(robots))\n",
        "#     q_table = np.zeros((len(states), len(actions)))\n",
        "\n",
        "#     for _ in range(episodes):\n",
        "#         for state in states:\n",
        "#             if random.uniform(0, 1) < epsilon:\n",
        "#                 action = random.choice(actions)\n",
        "#             else:\n",
        "#                 action = np.argmax(q_table[state])\n",
        "\n",
        "#             reward = aggregate_success(tasks[state], robots[action])\n",
        "#             next_state = (state + 1) % len(states)\n",
        "#             q_table[state, action] += alpha * (reward + gamma * np.max(q_table[next_state]) - q_table[state, action])\n",
        "\n",
        "#     total_success_rate = 0\n",
        "#     allocations = []\n",
        "#     for state in states:\n",
        "#         best_action = np.argmax(q_table[state])\n",
        "#         task_success_rate = aggregate_success(tasks[state], robots[best_action])\n",
        "#         total_success_rate += task_success_rate\n",
        "#         allocations.append((state, robots[best_action], task_success_rate))\n",
        "\n",
        "#     average_success_rate = total_success_rate / len(tasks)\n",
        "#     return average_success_rate, allocations\n",
        "\n",
        "# ------------------ DQN ------------------\n",
        "\n",
        "# def build_dqn_model(input_size, output_size):\n",
        "#     model = Sequential([\n",
        "#         Dense(24, input_dim=input_size, activation='relu'),\n",
        "#         Dense(24, activation='relu'),\n",
        "#         Dense(output_size, activation='linear')\n",
        "#     ])\n",
        "#     model.compile(optimizer=Adam(learning_rate=0.001), loss='mse')\n",
        "#     return model\n",
        "\n",
        "# def dqn_allocation(tasks, robots, episodes=10):\n",
        "#     state_size = len(tasks[0])\n",
        "#     action_size = len(robots)\n",
        "#     model = build_dqn_model(state_size, action_size)\n",
        "\n",
        "#     gamma = 0.9\n",
        "#     epsilon = 0.1\n",
        "#     batch_size = 32\n",
        "#     memory = []\n",
        "\n",
        "#     for episode in range(episodes):\n",
        "#         for task in tasks:\n",
        "#             state = np.array([len(task)])  # Simplified state representation\n",
        "#             if random.uniform(0, 1) < epsilon:\n",
        "#                 action = random.choice(range(action_size))\n",
        "#             else:\n",
        "#                 action = np.argmax(model.predict(state, verbose=0))\n",
        "\n",
        "#             reward = aggregate_success(task, robots[action])\n",
        "#             next_state = np.array([len(task)])  # Simplified next state\n",
        "#             memory.append((state, action, reward, next_state))\n",
        "\n",
        "#             if len(memory) > batch_size:\n",
        "#                 minibatch = random.sample(memory, batch_size)\n",
        "#                 for s, a, r, ns in minibatch:\n",
        "#                     target = r + gamma * np.max(model.predict(ns, verbose=0))\n",
        "#                     target_f = model.predict(s, verbose=0)\n",
        "#                     target_f[0][a] = target\n",
        "#                     model.fit(s, target_f, epochs=1, verbose=0)\n",
        "\n",
        "#     total_success_rate = 0\n",
        "#     allocations = []\n",
        "#     for task in tasks:\n",
        "#         state = np.array([len(task)])\n",
        "#         best_action = np.argmax(model.predict(state, verbose=0))\n",
        "#         task_success_rate = aggregate_success(task, robots[best_action])\n",
        "#         total_success_rate += task_success_rate\n",
        "#         allocations.append((task, robots[best_action], task_success_rate))\n",
        "\n",
        "#     average_success_rate = total_success_rate / len(tasks)\n",
        "#     return average_success_rate, allocations\n",
        "\n",
        "# ------------------ Execute and Compare ------------------\n",
        "\n",
        "# Run Q-Learning Allocation\n",
        "q_learning_result, q_learning_allocations = q_learning_allocation(subtasks, robots)\n",
        "print(f\"Q-Learning Success Rate: {q_learning_result:.2f}\")\n",
        "\n",
        "# Run DQN Allocation\n",
        "dqn_result, dqn_allocations = dqn_allocation(subtasks, robots)\n",
        "print(f\"DQN Success Rate: {dqn_result:.2f}\")\n",
        "\n",
        "# Visualization\n",
        "methods = ['Q-Learning', 'DQN']\n",
        "success_rates = [q_learning_result, dqn_result]\n",
        "\n",
        "plt.bar(methods, success_rates, color=['green', 'orange'])\n",
        "plt.title(\"Comparison of Success Rates for Q-Learning and DQN\")\n",
        "plt.ylabel(\"Average Success Rate\")\n",
        "plt.ylim(0, 1)\n",
        "plt.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "15iHU-yTkci2",
        "outputId": "fa4881f9-dd45-4dc0-bba8-6ef3bc9384da"
      },
      "execution_count": null,
      "outputs": [
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Episode 1/10 - Elapsed Time: 0.02s - Estimated Remaining Time: 0.17s\n",
            "Episode 2/10 - Elapsed Time: 0.04s - Estimated Remaining Time: 0.16s\n",
            "Episode 3/10 - Elapsed Time: 0.06s - Estimated Remaining Time: 0.14s\n",
            "Episode 4/10 - Elapsed Time: 0.08s - Estimated Remaining Time: 0.13s\n",
            "Episode 5/10 - Elapsed Time: 0.11s - Estimated Remaining Time: 0.11s\n",
            "Episode 6/10 - Elapsed Time: 0.12s - Estimated Remaining Time: 0.08s\n",
            "Episode 7/10 - Elapsed Time: 0.15s - Estimated Remaining Time: 0.06s\n",
            "Episode 8/10 - Elapsed Time: 0.17s - Estimated Remaining Time: 0.04s\n",
            "Episode 9/10 - Elapsed Time: 0.19s - Estimated Remaining Time: 0.02s\n",
            "Episode 10/10 - Elapsed Time: 0.21s - Estimated Remaining Time: 0.00s\n",
            "Q-Learning Success Rate: 0.71\n",
            "Episode 1/10 - Elapsed Time: 271.02s - Estimated Remaining Time: 2439.16s\n",
            "Episode 2/10 - Elapsed Time: 538.20s - Estimated Remaining Time: 2152.80s\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "iG88j0EGKu-Y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "pW_3nmfOKu7A"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "JDEtAi8XKu4B"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "tJFUFk1LKu2S"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "kRGtDlduKu04"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "gnMzzvc6KuzC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "-6nDXP9UKutz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "UzcutPtjKuo8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "vHZlsjrqKulg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "wEcEU8LiKuig"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "VeK6Qu88Kue6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "rs6NMhauKuVR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "\n",
        "# Placeholder function to simulate Q-Learning and DQN performance\n",
        "def simulate_performance_q_learning(param):\n",
        "    # Simulate some performance variation for Q-Learning\n",
        "    return np.sin(param) * 0.1 + 0.7 + np.random.uniform(-0.02, 0.02)\n",
        "\n",
        "def simulate_performance_dqn(param):\n",
        "    # Simulate some performance variation for DQN\n",
        "    return np.cos(param) * 0.1 + 0.75 + np.random.uniform(-0.02, 0.02)\n",
        "\n",
        "# Function to plot comparison graph\n",
        "def plot_comparison(x_values, q_learning_values, dqn_values, title, xlabel):\n",
        "    plt.figure(figsize=(10, 6))\n",
        "    plt.plot(x_values, q_learning_values, marker='o', label='Q-Learning', color='green')\n",
        "    plt.plot(x_values, dqn_values, marker='s', label='DQN', color='orange')\n",
        "    plt.title(title)\n",
        "    plt.xlabel(xlabel)\n",
        "    plt.ylabel('Average Success Rate')\n",
        "    plt.legend()\n",
        "    plt.grid(True)\n",
        "    plt.show()\n",
        "\n",
        "# ----------------- 1. Learning Rate (α) Variations -----------------\n",
        "learning_rates = np.linspace(0.01, 0.2, 10)\n",
        "q_learning_lr = [simulate_performance_q_learning(alpha) for alpha in learning_rates]\n",
        "dqn_lr = [simulate_performance_dqn(alpha) for alpha in learning_rates]\n",
        "\n",
        "plot_comparison(learning_rates, q_learning_lr, dqn_lr, 'Performance vs Learning Rate (α)', 'Learning Rate (α)')\n",
        "\n",
        "# ----------------- 2. Discount Factor (γ) Variations -----------------\n",
        "discount_factors = np.linspace(0.8, 0.99, 10)\n",
        "q_learning_gamma = [simulate_performance_q_learning(gamma) for gamma in discount_factors]\n",
        "dqn_gamma = [simulate_performance_dqn(gamma) for gamma in discount_factors]\n",
        "\n",
        "plot_comparison(discount_factors, q_learning_gamma, dqn_gamma, 'Performance vs Discount Factor (γ)', 'Discount Factor (γ)')\n",
        "\n",
        "# ----------------- 3. Number of Episodes -----------------\n",
        "episodes = np.arange(500, 5001, 500)\n",
        "q_learning_episodes = [simulate_performance_q_learning(ep) for ep in episodes]\n",
        "dqn_episodes = [simulate_performance_dqn(ep) for ep in episodes]\n",
        "\n",
        "plot_comparison(episodes, q_learning_episodes, dqn_episodes, 'Performance vs Number of Episodes', 'Number of Episodes')\n",
        "\n",
        "# ----------------- 4. Batch Size -----------------\n",
        "batch_sizes = np.array([16, 32, 64, 128, 256])\n",
        "q_learning_batch = [simulate_performance_q_learning(bs) for bs in batch_sizes]\n",
        "dqn_batch = [simulate_performance_dqn(bs) for bs in batch_sizes]\n",
        "\n",
        "plot_comparison(batch_sizes, q_learning_batch, dqn_batch, 'Performance vs Batch Size', 'Batch Size')\n",
        "\n",
        "# ----------------- 5. Exploration Rate (ε) Decay -----------------\n",
        "epsilons = np.linspace(0.99, 0.9, 10)\n",
        "q_learning_epsilon = [simulate_performance_q_learning(eps) for eps in epsilons]\n",
        "dqn_epsilon = [simulate_performance_dqn(eps) for eps in epsilons]\n",
        "\n",
        "plot_comparison(epsilons, q_learning_epsilon, dqn_epsilon, 'Performance vs Exploration Rate Decay (ε)', 'Exploration Rate (ε)')\n",
        "\n",
        "# ----------------- 6. Task Complexity -----------------\n",
        "task_complexity = np.array([1, 2, 3, 4, 5])  # Number of features per task\n",
        "q_learning_complexity = [simulate_performance_q_learning(tc) for tc in task_complexity]\n",
        "dqn_complexity = [simulate_performance_dqn(tc) for tc in task_complexity]\n",
        "\n",
        "plot_comparison(task_complexity, q_learning_complexity, dqn_complexity, 'Performance vs Task Complexity', 'Number of Features per Task')\n",
        "\n",
        "# ----------------- 7. Reward Scaling -----------------\n",
        "reward_scaling = np.array([0.5, 1.0, 1.5, 2.0])\n",
        "q_learning_reward = [simulate_performance_q_learning(rs) for rs in reward_scaling]\n",
        "dqn_reward = [simulate_performance_dqn(rs) for rs in reward_scaling]\n",
        "\n",
        "plot_comparison(reward_scaling, q_learning_reward, dqn_reward, 'Performance vs Reward Scaling', 'Reward Scaling Factor')\n",
        "\n",
        "# ----------------- 8. Robot Capability Variations -----------------\n",
        "robot_capability_scenarios = ['Light Dominant', 'Middle Balanced', 'Heavy Dominant']\n",
        "indices = np.arange(len(robot_capability_scenarios))\n",
        "q_learning_capability = [simulate_performance_q_learning(i) for i in indices]\n",
        "dqn_capability = [simulate_performance_dqn(i) for i in indices]\n",
        "\n",
        "plt.figure(figsize=(10, 6))\n",
        "plt.bar(indices - 0.2, q_learning_capability, width=0.4, label='Q-Learning', color='green')\n",
        "plt.bar(indices + 0.2, dqn_capability, width=0.4, label='DQN', color='orange')\n",
        "plt.xticks(indices, robot_capability_scenarios)\n",
        "plt.title('Performance vs Robot Capability Variations')\n",
        "plt.xlabel('Robot Capability Scenario')\n",
        "plt.ylabel('Average Success Rate')\n",
        "plt.legend()\n",
        "plt.grid(True)\n",
        "plt.show()\n",
        "\n",
        "# ----------------- 9. Noise and Uncertainty -----------------\n",
        "noise_levels = np.linspace(0.01, 0.1, 10)\n",
        "q_learning_noise = [simulate_performance_q_learning(nl) for nl in noise_levels]\n",
        "dqn_noise = [simulate_performance_dqn(nl) for nl in noise_levels]\n",
        "\n",
        "plot_comparison(noise_levels, q_learning_noise, dqn_noise, 'Performance vs Noise Level', 'Noise Standard Deviation')\n",
        "\n",
        "# ----------------- 10. Training Time per Episode -----------------\n",
        "training_times = np.linspace(0.1, 1.0, 10)\n",
        "q_learning_time = [simulate_performance_q_learning(tt) for tt in training_times]\n",
        "dqn_time = [simulate_performance_dqn(tt) for tt in training_times]\n",
        "\n",
        "plot_comparison(training_times, q_learning_time, dqn_time, 'Performance vs Training Time per Episode', 'Training Time (seconds)')\n"
      ],
      "metadata": {
        "id": "C9REFuUNlT5q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 420
        },
        "id": "QGy6fJxAr9dp",
        "outputId": "e947feea-adee-41ff-deb6-667c6d330de8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/keras/src/layers/core/dense.py:87: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
            "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-19-ffd2e0542384>\u001b[0m in \u001b[0;36m<cell line: 114>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    112\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    113\u001b[0m \u001b[0mq_learning_results\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0msimulate_performance_q_learning\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0me\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mepisodes\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 114\u001b[0;31m \u001b[0mdqn_results\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0msimulate_performance_dqn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0me\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mepisodes\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    115\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    116\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfigure\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfigsize\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m6\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-19-ffd2e0542384>\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    112\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    113\u001b[0m \u001b[0mq_learning_results\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0msimulate_performance_q_learning\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0me\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mepisodes\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 114\u001b[0;31m \u001b[0mdqn_results\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0msimulate_performance_dqn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0me\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mepisodes\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    115\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    116\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfigure\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfigsize\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m6\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-19-ffd2e0542384>\u001b[0m in \u001b[0;36msimulate_performance_dqn\u001b[0;34m(param)\u001b[0m\n\u001b[1;32m    106\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    107\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0msimulate_performance_dqn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparam\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 108\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mdqn_allocation\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msubtasks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrobots\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepisodes\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mparam\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    109\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    110\u001b[0m \u001b[0;31m# ------------------ Comparison Graphs ------------------\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-19-ffd2e0542384>\u001b[0m in \u001b[0;36mdqn_allocation\u001b[0;34m(tasks, robots, episodes, batch_size, gamma, epsilon)\u001b[0m\n\u001b[1;32m     90\u001b[0m                     \u001b[0mtargets\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mr\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mgamma\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mns\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     91\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 92\u001b[0;31m                 \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstates\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtargets\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     93\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     94\u001b[0m     \u001b[0mtotal_success_rate\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/keras/src/utils/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    115\u001b[0m         \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    116\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 117\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    118\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    119\u001b[0m             \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/keras/src/backend/tensorflow/trainer.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq)\u001b[0m\n\u001b[1;32m    280\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    281\u001b[0m         \u001b[0;31m# Create an iterator that yields batches for one epoch.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 282\u001b[0;31m         epoch_iterator = TFEpochIterator(\n\u001b[0m\u001b[1;32m    283\u001b[0m             \u001b[0mx\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    284\u001b[0m             \u001b[0my\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/keras/src/backend/tensorflow/trainer.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, distribute_strategy, *args, **kwargs)\u001b[0m\n\u001b[1;32m    666\u001b[0m         \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    667\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_distribute_strategy\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdistribute_strategy\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 668\u001b[0;31m         \u001b[0mdataset\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_iterator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    669\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdistribute\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDistributedDataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    670\u001b[0m             dataset = self._distribute_strategy.experimental_distribute_dataset(\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/keras/src/backend/tensorflow/trainer.py\u001b[0m in \u001b[0;36m_get_iterator\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    675\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    676\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_get_iterator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 677\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata_adapter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_tf_dataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    678\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    679\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0menumerate_epoch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/keras/src/trainers/data_adapters/array_data_adapter.py\u001b[0m in \u001b[0;36mget_tf_dataset\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    230\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mdataset\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    231\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 232\u001b[0;31m         \u001b[0mindices_dataset\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mindices_dataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mflat_map\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mslice_batch_indices\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    233\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mshuffle\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"batch\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    234\u001b[0m             \u001b[0mindices_dataset\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mindices_dataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshuffle\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/data/ops/dataset_ops.py\u001b[0m in \u001b[0;36mflat_map\u001b[0;34m(self, map_func, name)\u001b[0m\n\u001b[1;32m   2354\u001b[0m     \u001b[0;31m# pylint: disable=g-import-not-at-top,protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2355\u001b[0m     \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mops\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mflat_map_op\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2356\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mflat_map_op\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_flat_map\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmap_func\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2357\u001b[0m     \u001b[0;31m# pylint: enable=g-import-not-at-top,protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2358\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/data/ops/flat_map_op.py\u001b[0m in \u001b[0;36m_flat_map\u001b[0;34m(input_dataset, map_func, name)\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0m_flat_map\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_dataset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmap_func\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# pylint: disable=unused-private-name\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m   \u001b[0;34m\"\"\"See `Dataset.flat_map()` for details.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 24\u001b[0;31m   \u001b[0;32mreturn\u001b[0m \u001b[0m_FlatMapDataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_dataset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmap_func\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     25\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/data/ops/flat_map_op.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, input_dataset, map_func, name)\u001b[0m\n\u001b[1;32m     31\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     32\u001b[0m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_input_dataset\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minput_dataset\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 33\u001b[0;31m     self._map_func = structured_function.StructuredFunctionWrapper(\n\u001b[0m\u001b[1;32m     34\u001b[0m         map_func, self._transformation_name(), dataset=input_dataset)\n\u001b[1;32m     35\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_map_func\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutput_structure\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdataset_ops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDatasetSpec\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/data/ops/structured_function.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, func, transformation_name, dataset, input_classes, input_shapes, input_types, input_structure, add_to_graph, use_legacy_function, defun_kwargs)\u001b[0m\n\u001b[1;32m    263\u001b[0m         \u001b[0mfn_factory\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrace_tf_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdefun_kwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    264\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 265\u001b[0;31m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_function\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfn_factory\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    266\u001b[0m     \u001b[0;31m# There is no graph to add in eager mode.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    267\u001b[0m     \u001b[0madd_to_graph\u001b[0m \u001b[0;34m&=\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mcontext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexecuting_eagerly\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/eager/polymorphic_function/polymorphic_function.py\u001b[0m in \u001b[0;36mget_concrete_function\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1249\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mget_concrete_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1250\u001b[0m     \u001b[0;31m# Implements PolymorphicFunction.get_concrete_function.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1251\u001b[0;31m     \u001b[0mconcrete\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_concrete_function_garbage_collected\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1252\u001b[0m     \u001b[0mconcrete\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_garbage_collector\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrelease\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1253\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mconcrete\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/eager/polymorphic_function/polymorphic_function.py\u001b[0m in \u001b[0;36m_get_concrete_function_garbage_collected\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1219\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_variable_creation_config\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1220\u001b[0m         \u001b[0minitializers\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1221\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_initialize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0madd_initializers_to\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minitializers\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1222\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_initialize_uninitialized_variables\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minitializers\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1223\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/eager/polymorphic_function/polymorphic_function.py\u001b[0m in \u001b[0;36m_initialize\u001b[0;34m(self, args, kwds, add_initializers_to)\u001b[0m\n\u001b[1;32m    694\u001b[0m     )\n\u001b[1;32m    695\u001b[0m     \u001b[0;31m# Force the definition of the function for these arguments\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 696\u001b[0;31m     self._concrete_variable_creation_fn = tracing_compilation.trace_function(\n\u001b[0m\u001b[1;32m    697\u001b[0m         \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_variable_creation_config\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    698\u001b[0m     )\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/eager/polymorphic_function/tracing_compilation.py\u001b[0m in \u001b[0;36mtrace_function\u001b[0;34m(args, kwargs, tracing_options)\u001b[0m\n\u001b[1;32m    176\u001b[0m       \u001b[0mkwargs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    177\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 178\u001b[0;31m     concrete_function = _maybe_define_function(\n\u001b[0m\u001b[1;32m    179\u001b[0m         \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtracing_options\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    180\u001b[0m     )\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/eager/polymorphic_function/tracing_compilation.py\u001b[0m in \u001b[0;36m_maybe_define_function\u001b[0;34m(args, kwargs, tracing_options)\u001b[0m\n\u001b[1;32m    281\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    282\u001b[0m           \u001b[0mtarget_func_type\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlookup_func_type\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 283\u001b[0;31m         concrete_function = _create_concrete_function(\n\u001b[0m\u001b[1;32m    284\u001b[0m             \u001b[0mtarget_func_type\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlookup_func_context\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfunc_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtracing_options\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    285\u001b[0m         )\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/eager/polymorphic_function/tracing_compilation.py\u001b[0m in \u001b[0;36m_create_concrete_function\u001b[0;34m(function_type, type_context, func_graph, tracing_options)\u001b[0m\n\u001b[1;32m    308\u001b[0m       \u001b[0mattributes_lib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDISABLE_ACD\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    309\u001b[0m   )\n\u001b[0;32m--> 310\u001b[0;31m   traced_func_graph = func_graph_module.func_graph_from_py_func(\n\u001b[0m\u001b[1;32m    311\u001b[0m       \u001b[0mtracing_options\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    312\u001b[0m       \u001b[0mtracing_options\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpython_function\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/framework/func_graph.py\u001b[0m in \u001b[0;36mfunc_graph_from_py_func\u001b[0;34m(name, python_func, args, kwargs, signature, func_graph, add_control_dependencies, arg_names, op_return_value, collections, capture_by_value, create_placeholders)\u001b[0m\n\u001b[1;32m   1015\u001b[0m     \u001b[0;31m# TensorSpecs by their `arg_names` for later binding.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1016\u001b[0m     func_graph.structured_input_signature = (\n\u001b[0;32m-> 1017\u001b[0;31m         convert_structure_to_signature(\n\u001b[0m\u001b[1;32m   1018\u001b[0m             func_args, arg_names, signature_context=signature_context),\n\u001b[1;32m   1019\u001b[0m         convert_structure_to_signature(\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/framework/func_graph.py\u001b[0m in \u001b[0;36mconvert_structure_to_signature\u001b[0;34m(structure, arg_names, signature_context)\u001b[0m\n\u001b[1;32m    128\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    129\u001b[0m   \u001b[0mmapped\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mencode_arg\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mpath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0marg\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mflattened\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 130\u001b[0;31m   \u001b[0;32mreturn\u001b[0m \u001b[0mnest\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpack_sequence_as\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstructure\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmapped\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    131\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    132\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/util/nest.py\u001b[0m in \u001b[0;36mpack_sequence_as\u001b[0;34m(structure, flat_sequence, expand_composites)\u001b[0m\n\u001b[1;32m    535\u001b[0m     \u001b[0mTypeError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0mstructure\u001b[0m\u001b[0;31m`\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mcontains\u001b[0m \u001b[0ma\u001b[0m \u001b[0mdict\u001b[0m \u001b[0;32mwith\u001b[0m \u001b[0mnon\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0msortable\u001b[0m \u001b[0mkeys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    536\u001b[0m   \"\"\"\n\u001b[0;32m--> 537\u001b[0;31m   return nest_util.pack_sequence_as(\n\u001b[0m\u001b[1;32m    538\u001b[0m       \u001b[0mnest_util\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mModality\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mCORE\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstructure\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mflat_sequence\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexpand_composites\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    539\u001b[0m   )\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/util/nest_util.py\u001b[0m in \u001b[0;36mpack_sequence_as\u001b[0;34m(modality, structure, flat_sequence, expand_composites, sequence_fn)\u001b[0m\n\u001b[1;32m    853\u001b[0m   \"\"\"\n\u001b[1;32m    854\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0mmodality\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mModality\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mCORE\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 855\u001b[0;31m     return _tf_core_pack_sequence_as(\n\u001b[0m\u001b[1;32m    856\u001b[0m         \u001b[0mstructure\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mflat_sequence\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexpand_composites\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msequence_fn\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    857\u001b[0m     )\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/util/nest_util.py\u001b[0m in \u001b[0;36m_tf_core_pack_sequence_as\u001b[0;34m(structure, flat_sequence, expand_composites, sequence_fn)\u001b[0m\n\u001b[1;32m    917\u001b[0m           \u001b[0;34m%\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mflat_structure\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mflat_sequence\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstructure\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mflat_sequence\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    918\u001b[0m       )\n\u001b[0;32m--> 919\u001b[0;31m   \u001b[0;32mreturn\u001b[0m \u001b[0msequence_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstructure\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpacked\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    920\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    921\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/util/nest_util.py\u001b[0m in \u001b[0;36msequence_like\u001b[0;34m(instance, args)\u001b[0m\n\u001b[1;32m    242\u001b[0m     \u001b[0;31m# in the proxy type.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    243\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minstance\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msequence_like\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minstance\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__wrapped__\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 244\u001b[0;31m   \u001b[0;32melif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minstance\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mCustomNestProtocol\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    245\u001b[0m     \u001b[0mmetadata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minstance\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__tf_flatten__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    246\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0minstance\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__tf_unflatten__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmetadata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtuple\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.10/typing.py\u001b[0m in \u001b[0;36m__instancecheck__\u001b[0;34m(cls, instance)\u001b[0m\n\u001b[1;32m   1510\u001b[0m                     for attr in _get_protocol_attrs(cls)):\n\u001b[1;32m   1511\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1512\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__instancecheck__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minstance\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1513\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1514\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.10/abc.py\u001b[0m in \u001b[0;36m__instancecheck__\u001b[0;34m(cls, instance)\u001b[0m\n\u001b[1;32m    117\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0m__instancecheck__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcls\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minstance\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    118\u001b[0m             \u001b[0;34m\"\"\"Override for isinstance(instance, cls).\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 119\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0m_abc_instancecheck\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcls\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minstance\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    120\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    121\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0m__subclasscheck__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcls\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msubclass\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import random\n",
        "import matplotlib.pyplot as plt\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from collections import deque\n",
        "\n",
        "# ------------------ Load Task Data ------------------\n",
        "file_path = 'annotated_actions.csv'  # Ensure this file exists\n",
        "actions_df = pd.read_csv(file_path)\n",
        "actions_df['features'] = actions_df['features'].fillna('').astype(str)\n",
        "subtasks = actions_df['features'].apply(lambda x: x.split(', ') if x else []).tolist()\n",
        "\n",
        "# ------------------ Define Success Matrix and Robots ------------------\n",
        "success_matrix = {\n",
        "    'careful': {'light': 0.9, 'middle': 0.7, 'heavy': 0.5},\n",
        "    'dexterous': {'light': 0.8, 'middle': 0.6, 'heavy': 0.4},\n",
        "    'heavy': {'light': 0.5, 'middle': 0.7, 'heavy': 0.9}\n",
        "}\n",
        "robots = ['light', 'middle', 'heavy']\n",
        "\n",
        "def aggregate_success(features, robot_type):\n",
        "    return sum(success_matrix[feature][robot_type] for feature in features) / len(features) if features else 0\n",
        "\n",
        "# ------------------ Q-Learning ------------------\n",
        "def q_learning_allocation(tasks, robots, alpha=0.1, gamma=0.9, epsilon=0.1, episodes=100):\n",
        "    states = range(len(tasks))\n",
        "    actions = range(len(robots))\n",
        "    q_table = np.zeros((len(states), len(actions)))\n",
        "\n",
        "    for _ in range(episodes):\n",
        "        for state in states:\n",
        "            if random.uniform(0, 1) < epsilon:\n",
        "                action = random.choice(actions)\n",
        "            else:\n",
        "                action = np.argmax(q_table[state])\n",
        "\n",
        "            reward = aggregate_success(tasks[state], robots[action])\n",
        "            next_state = (state + 1) % len(states)\n",
        "            q_table[state, action] += alpha * (reward + gamma * np.max(q_table[next_state]) - q_table[state, action])\n",
        "\n",
        "    total_success_rate = 0\n",
        "    for state in states:\n",
        "        best_action = np.argmax(q_table[state])\n",
        "        task_success_rate = aggregate_success(tasks[state], robots[best_action])\n",
        "        total_success_rate += task_success_rate\n",
        "\n",
        "    return total_success_rate / len(tasks)\n",
        "\n",
        "# ------------------ DQN ------------------\n",
        "def build_dqn_model(input_size, output_size):\n",
        "    model = Sequential([\n",
        "        Dense(24, input_shape=(input_size,), activation='relu'),\n",
        "        Dense(24, activation='relu'),\n",
        "        Dense(output_size, activation='linear')\n",
        "    ])\n",
        "    model.compile(optimizer=Adam(learning_rate=0.001), loss='mse')\n",
        "    return model\n",
        "\n",
        "def dqn_allocation(tasks, robots, alpha=0.1, gamma=0.9, epsilon=0.1, episodes=100, batch_size=32):\n",
        "    # Here, alpha is not directly used in the default DQN (we use a fixed learning rate in the optimizer),\n",
        "    # but we include it for consistency if you want to modify the optimizer learning rate dynamically.\n",
        "    state_size = 1\n",
        "    action_size = len(robots)\n",
        "    model = build_dqn_model(state_size, action_size)\n",
        "    memory = deque(maxlen=2000)\n",
        "\n",
        "    # If you want to adjust learning rate based on alpha:\n",
        "    # model.optimizer.learning_rate = alpha\n",
        "\n",
        "    for episode in range(episodes):\n",
        "        for task in tasks:\n",
        "            state = np.array([[len(task)]], dtype=np.float32)\n",
        "            if random.uniform(0, 1) < epsilon:\n",
        "                action = random.choice(range(action_size))\n",
        "            else:\n",
        "                q_values = model.predict(state, verbose=0)\n",
        "                action = np.argmax(q_values[0])\n",
        "\n",
        "            reward = aggregate_success(task, robots[action])\n",
        "            next_state = np.array([[len(task)]], dtype=np.float32)\n",
        "            memory.append((state, action, reward, next_state))\n",
        "\n",
        "            if len(memory) > batch_size:\n",
        "                minibatch = random.sample(memory, batch_size)\n",
        "                states = np.vstack([s for s, _, _, _ in minibatch])\n",
        "                targets = model.predict(states, verbose=0)\n",
        "\n",
        "                # Update targets\n",
        "                for i, (s, a, r, ns) in enumerate(minibatch):\n",
        "                    t = targets[i]\n",
        "                    t[a] = r + gamma * np.max(model.predict(ns, verbose=0))\n",
        "\n",
        "                model.fit(states, targets, epochs=1, verbose=0)\n",
        "\n",
        "    total_success_rate = 0\n",
        "    for task in tasks:\n",
        "        state = np.array([[len(task)]], dtype=np.float32)\n",
        "        q_values = model.predict(state, verbose=0)\n",
        "        best_action = np.argmax(q_values[0])\n",
        "        task_success_rate = aggregate_success(task, robots[best_action])\n",
        "        total_success_rate += task_success_rate\n",
        "\n",
        "    return total_success_rate / len(tasks)\n",
        "\n",
        "# ------------------ Plotting Helper ------------------\n",
        "def plot_comparison(x_values, q_learning_values, dqn_values, title, xlabel):\n",
        "    plt.figure(figsize=(10, 6))\n",
        "    plt.plot(x_values, q_learning_values, marker='o', label='Q-Learning', color='green')\n",
        "    plt.plot(x_values, dqn_values, marker='s', label='DQN', color='orange')\n",
        "    plt.title(title)\n",
        "    plt.xlabel(xlabel)\n",
        "    plt.ylabel('Average Success Rate')\n",
        "    plt.legend()\n",
        "    plt.grid(True)\n",
        "    plt.show()\n",
        "\n",
        "# ------------------ Parameter Variation Simulations ------------------\n",
        "# We define separate simulation functions for each parameter variation to set them correctly.\n",
        "\n",
        "# 1. Learning Rate (α) Variations\n",
        "def simulate_performance_q_learning_alpha(alpha):\n",
        "    return q_learning_allocation(subtasks, robots, alpha=alpha, episodes=100) # fix episodes for fair comparison\n",
        "\n",
        "def simulate_performance_dqn_alpha(alpha):\n",
        "    # Using alpha to hypothetically adjust learning rate of DQN\n",
        "    # One way is to set episodes fixed:\n",
        "    return dqn_allocation(subtasks, robots, alpha=alpha, episodes=100, batch_size=32)\n",
        "\n",
        "learning_rates = np.linspace(0.01, 0.2, 10)\n",
        "q_learning_lr = [simulate_performance_q_learning_alpha(a) for a in learning_rates]\n",
        "dqn_lr = [simulate_performance_dqn_alpha(a) for a in learning_rates]\n",
        "\n",
        "plot_comparison(learning_rates, q_learning_lr, dqn_lr, 'Performance vs Learning Rate (α)', 'Learning Rate (α)')\n",
        "\n",
        "# 2. Discount Factor (γ) Variations\n",
        "def simulate_performance_q_learning_gamma(gamma):\n",
        "    return q_learning_allocation(subtasks, robots, gamma=gamma, episodes=100)\n",
        "\n",
        "def simulate_performance_dqn_gamma(gamma):\n",
        "    return dqn_allocation(subtasks, robots, gamma=gamma, episodes=100, batch_size=32)\n",
        "\n",
        "discount_factors = np.linspace(0.8, 0.99, 10)\n",
        "q_learning_gamma = [simulate_performance_q_learning_gamma(g) for g in discount_factors]\n",
        "dqn_gamma = [simulate_performance_dqn_gamma(g) for g in discount_factors]\n",
        "\n",
        "plot_comparison(discount_factors, q_learning_gamma, dqn_gamma, 'Performance vs Discount Factor (γ)', 'Discount Factor (γ)')\n",
        "\n",
        "# 3. Number of Episodes\n",
        "def simulate_performance_q_learning_episodes(ep):\n",
        "    return q_learning_allocation(subtasks, robots, episodes=ep)\n",
        "\n",
        "def simulate_performance_dqn_episodes(ep):\n",
        "    return dqn_allocation(subtasks, robots, episodes=ep, batch_size=32)\n",
        "\n",
        "episodes = np.arange(500, 5001, 500)\n",
        "q_learning_episodes = [simulate_performance_q_learning_episodes(ep) for ep in episodes]\n",
        "dqn_episodes = [simulate_performance_dqn_episodes(ep) for ep in episodes]\n",
        "\n",
        "plot_comparison(episodes, q_learning_episodes, dqn_episodes, 'Performance vs Number of Episodes', 'Number of Episodes')\n",
        "\n",
        "# 4. Batch Size (applies mainly to DQN)\n",
        "def simulate_performance_q_learning_batch(bs):\n",
        "    # Q-Learning does not use batch size, so we just ignore it\n",
        "    return q_learning_allocation(subtasks, robots, episodes=100)\n",
        "\n",
        "def simulate_performance_dqn_batch(bs):\n",
        "    return dqn_allocation(subtasks, robots, episodes=100, batch_size=bs)\n",
        "\n",
        "batch_sizes = np.array([16, 32, 64, 128, 256])\n",
        "q_learning_batch = [simulate_performance_q_learning_batch(bs) for bs in batch_sizes]\n",
        "dqn_batch = [simulate_performance_dqn_batch(bs) for bs in batch_sizes]\n",
        "\n",
        "plot_comparison(batch_sizes, q_learning_batch, dqn_batch, 'Performance vs Batch Size', 'Batch Size')\n",
        "\n",
        "# 5. Exploration Rate (ε) Decay\n",
        "# We'll treat param as epsilon here\n",
        "def simulate_performance_q_learning_epsilon(eps):\n",
        "    return q_learning_allocation(subtasks, robots, epsilon=eps, episodes=100)\n",
        "\n",
        "def simulate_performance_dqn_epsilon(eps):\n",
        "    return dqn_allocation(subtasks, robots, epsilon=eps, episodes=100, batch_size=32)\n",
        "\n",
        "epsilons = np.linspace(0.99, 0.9, 10)\n",
        "q_learning_epsilon = [simulate_performance_q_learning_epsilon(eps) for eps in epsilons]\n",
        "dqn_epsilon = [simulate_performance_dqn_epsilon(eps) for eps in epsilons]\n",
        "\n",
        "plot_comparison(epsilons, q_learning_epsilon, dqn_epsilon, 'Performance vs Exploration Rate Decay (ε)', 'Exploration Rate (ε)')\n",
        "\n",
        "# 6. Task Complexity (Assuming complexity = number of features per task)\n",
        "# Here we must simulate tasks with different complexities. We'll create dummy tasks for this test.\n",
        "def create_tasks_with_complexity(num_features, n_tasks=20):\n",
        "    features_list = list(success_matrix.keys())  # ['careful','dexterous','heavy']\n",
        "    tasks = []\n",
        "    for _ in range(n_tasks):\n",
        "        task_features = random.choices(features_list, k=num_features)\n",
        "        tasks.append(task_features)\n",
        "    return tasks\n",
        "\n",
        "def simulate_performance_q_learning_complexity(num_features):\n",
        "    complex_tasks = create_tasks_with_complexity(num_features)\n",
        "    return q_learning_allocation(complex_tasks, robots, episodes=100)\n",
        "\n",
        "def simulate_performance_dqn_complexity(num_features):\n",
        "    complex_tasks = create_tasks_with_complexity(num_features)\n",
        "    return dqn_allocation(complex_tasks, robots, episodes=100, batch_size=32)\n",
        "\n",
        "task_complexity = np.array([1, 2, 3, 4, 5])\n",
        "q_learning_complexity = [simulate_performance_q_learning_complexity(tc) for tc in task_complexity]\n",
        "dqn_complexity = [simulate_performance_dqn_complexity(tc) for tc in task_complexity]\n",
        "\n",
        "plot_comparison(task_complexity, q_learning_complexity, dqn_complexity, 'Performance vs Task Complexity', 'Number of Features per Task')\n",
        "\n",
        "# 7. Reward Scaling (We'll just multiply the reward by this factor)\n",
        "def q_learning_allocation_scaled(tasks, robots, alpha=0.1, gamma=0.9, epsilon=0.1, episodes=100, scale=1.0):\n",
        "    states = range(len(tasks))\n",
        "    actions = range(len(robots))\n",
        "    q_table = np.zeros((len(states), len(actions)))\n",
        "\n",
        "    for _ in range(episodes):\n",
        "        for state in states:\n",
        "            if random.uniform(0, 1) < epsilon:\n",
        "                action = random.choice(actions)\n",
        "            else:\n",
        "                action = np.argmax(q_table[state])\n",
        "            reward = aggregate_success(tasks[state], robots[action]) * scale\n",
        "            next_state = (state + 1) % len(states)\n",
        "            q_table[state, action] += alpha * (reward + gamma * np.max(q_table[next_state]) - q_table[state, action])\n",
        "\n",
        "    total_success_rate = 0\n",
        "    for state in states:\n",
        "        best_action = np.argmax(q_table[state])\n",
        "        task_success_rate = aggregate_success(tasks[state], robots[best_action])\n",
        "        total_success_rate += task_success_rate\n",
        "    return total_success_rate / len(tasks)\n",
        "\n",
        "def dqn_allocation_scaled(tasks, robots, alpha=0.1, gamma=0.9, epsilon=0.1, episodes=100, batch_size=32, scale=1.0):\n",
        "    state_size = 1\n",
        "    action_size = len(robots)\n",
        "    model = build_dqn_model(state_size, action_size)\n",
        "    memory = deque(maxlen=2000)\n",
        "\n",
        "    for episode in range(episodes):\n",
        "        for task in tasks:\n",
        "            state = np.array([[len(task)]], dtype=np.float32)\n",
        "            if random.uniform(0, 1) < epsilon:\n",
        "                action = random.choice(range(action_size))\n",
        "            else:\n",
        "                q_values = model.predict(state, verbose=0)\n",
        "                action = np.argmax(q_values[0])\n",
        "\n",
        "            reward = aggregate_success(task, robots[action]) * scale\n",
        "            next_state = np.array([[len(task)]], dtype=np.float32)\n",
        "            memory.append((state, action, reward, next_state))\n",
        "\n",
        "            if len(memory) > batch_size:\n",
        "                minibatch = random.sample(memory, batch_size)\n",
        "                states = np.vstack([s for s, _, _, _ in minibatch])\n",
        "                targets = model.predict(states, verbose=0)\n",
        "                for i, (s, a, r, ns) in enumerate(minibatch):\n",
        "                    t = targets[i]\n",
        "                    t[a] = r + gamma * np.max(model.predict(ns, verbose=0))\n",
        "                model.fit(states, targets, epochs=1, verbose=0)\n",
        "\n",
        "    total_success_rate = 0\n",
        "    for task in tasks:\n",
        "        state = np.array([[len(task)]], dtype=np.float32)\n",
        "        q_values = model.predict(state, verbose=0)\n",
        "        best_action = np.argmax(q_values[0])\n",
        "        task_success_rate = aggregate_success(task, robots[best_action])\n",
        "        total_success_rate += task_success_rate\n",
        "\n",
        "    return total_success_rate / len(tasks)\n",
        "\n",
        "def simulate_performance_q_learning_reward(scale):\n",
        "    return q_learning_allocation_scaled(subtasks, robots, scale=scale, episodes=100)\n",
        "\n",
        "def simulate_performance_dqn_reward(scale):\n",
        "    return dqn_allocation_scaled(subtasks, robots, scale=scale, episodes=100, batch_size=32)\n",
        "\n",
        "reward_scaling = np.array([0.5, 1.0, 1.5, 2.0])\n",
        "q_learning_reward = [simulate_performance_q_learning_reward(rs) for rs in reward_scaling]\n",
        "dqn_reward = [simulate_performance_dqn_reward(rs) for rs in reward_scaling]\n",
        "\n",
        "plot_comparison(reward_scaling, q_learning_reward, dqn_reward, 'Performance vs Reward Scaling', 'Reward Scaling Factor')\n",
        "\n",
        "# 8. Robot Capability Variations (This is a scenario-based test, you would adjust success_matrix or tasks)\n",
        "# We'll just simulate different robot sets or success matrices. For simplicity, we will just run with indices as placeholders.\n",
        "def simulate_performance_q_learning_capability(idx):\n",
        "    # In a real scenario, you'd modify robots or success_matrix here based on idx.\n",
        "    return q_learning_allocation(subtasks, robots, episodes=100)\n",
        "\n",
        "def simulate_performance_dqn_capability(idx):\n",
        "    return dqn_allocation(subtasks, robots, episodes=100, batch_size=32)\n",
        "\n",
        "robot_capability_scenarios = ['Light Dominant', 'Middle Balanced', 'Heavy Dominant']\n",
        "indices = np.arange(len(robot_capability_scenarios))\n",
        "q_learning_capability = [simulate_performance_q_learning_capability(i) for i in indices]\n",
        "dqn_capability = [simulate_performance_dqn_capability(i) for i in indices]\n",
        "\n",
        "plt.figure(figsize=(10, 6))\n",
        "plt.bar(indices - 0.2, q_learning_capability, width=0.4, label='Q-Learning', color='green')\n",
        "plt.bar(indices + 0.2, dqn_capability, width=0.4, label='DQN', color='orange')\n",
        "plt.xticks(indices, robot_capability_scenarios)\n",
        "plt.title('Performance vs Robot Capability Variations')\n",
        "plt.xlabel('Robot Capability Scenario')\n",
        "plt.ylabel('Average Success Rate')\n",
        "plt.legend()\n",
        "plt.grid(True)\n",
        "plt.show()\n",
        "\n",
        "# 9. Noise and Uncertainty\n",
        "# We can simulate noise by perturbing rewards.\n",
        "def simulate_performance_q_learning_noise(noise_level):\n",
        "    noisy_tasks = []\n",
        "    for task in subtasks:\n",
        "        # Add noise to success probabilities\n",
        "        # We'll do this by adjusting the success probability calculation dynamically\n",
        "        # Instead of rewriting q_learning_allocation, just create tasks with the same features.\n",
        "        # We'll handle noise inside the allocation temporarily.\n",
        "        return q_learning_allocation_noisy(subtasks, robots, noise_level=noise_level, episodes=100)\n",
        "\n",
        "def simulate_performance_dqn_noise(noise_level):\n",
        "    return dqn_allocation_noisy(subtasks, robots, noise_level=noise_level, episodes=100, batch_size=32)\n",
        "\n",
        "def q_learning_allocation_noisy(tasks, robots, alpha=0.1, gamma=0.9, epsilon=0.1, episodes=100, noise_level=0.0):\n",
        "    states = range(len(tasks))\n",
        "    actions = range(len(robots))\n",
        "    q_table = np.zeros((len(states), len(actions)))\n",
        "\n",
        "    for _ in range(episodes):\n",
        "        for state in states:\n",
        "            if random.uniform(0, 1) < epsilon:\n",
        "                action = random.choice(actions)\n",
        "            else:\n",
        "                action = np.argmax(q_table[state])\n",
        "            base_reward = aggregate_success(tasks[state], robots[action])\n",
        "            # Add noise\n",
        "            reward = base_reward + np.random.normal(0, noise_level)\n",
        "            reward = max(0, min(1, reward))  # clip between 0 and 1\n",
        "            next_state = (state + 1) % len(states)\n",
        "            q_table[state, action] += alpha * (reward + gamma * np.max(q_table[next_state]) - q_table[state, action])\n",
        "\n",
        "    total_success_rate = 0\n",
        "    for state in states:\n",
        "        best_action = np.argmax(q_table[state])\n",
        "        task_success_rate = aggregate_success(tasks[state], robots[best_action])\n",
        "        total_success_rate += task_success_rate\n",
        "    return total_success_rate / len(tasks)\n",
        "\n",
        "def dqn_allocation_noisy(tasks, robots, alpha=0.1, gamma=0.9, epsilon=0.1, episodes=100, batch_size=32, noise_level=0.0):\n",
        "    state_size = 1\n",
        "    action_size = len(robots)\n",
        "    model = build_dqn_model(state_size, action_size)\n",
        "    memory = deque(maxlen=2000)\n",
        "\n",
        "    for episode in range(episodes):\n",
        "        for task in tasks:\n",
        "            state = np.array([[len(task)]], dtype=np.float32)\n",
        "            if random.uniform(0, 1) < epsilon:\n",
        "                action = random.choice(range(action_size))\n",
        "            else:\n",
        "                q_values = model.predict(state, verbose=0)\n",
        "                action = np.argmax(q_values[0])\n",
        "\n",
        "            base_reward = aggregate_success(task, robots[action])\n",
        "            reward = base_reward + np.random.normal(0, noise_level)\n",
        "            reward = max(0, min(1, reward))\n",
        "            next_state = np.array([[len(task)]], dtype=np.float32)\n",
        "            memory.append((state, action, reward, next_state))\n",
        "\n",
        "            if len(memory) > batch_size:\n",
        "                minibatch = random.sample(memory, batch_size)\n",
        "                states = np.vstack([s for s, _, _, _ in minibatch])\n",
        "                targets = model.predict(states, verbose=0)\n",
        "                for i, (s, a, r, ns) in enumerate(minibatch):\n",
        "                    t = targets[i]\n",
        "                    t[a] = r + gamma * np.max(model.predict(ns, verbose=0))\n",
        "                model.fit(states, targets, epochs=1, verbose=0)\n",
        "\n",
        "    total_success_rate = 0\n",
        "    for task in tasks:\n",
        "        state = np.array([[len(task)]], dtype=np.float32)\n",
        "        q_values = model.predict(state, verbose=0)\n",
        "        best_action = np.argmax(q_values[0])\n",
        "        task_success_rate = aggregate_success(task, robots[best_action])\n",
        "        total_success_rate += task_success_rate\n",
        "    return total_success_rate / len(tasks)\n",
        "\n",
        "noise_levels = np.linspace(0.01, 0.1, 10)\n",
        "q_learning_noise = [simulate_performance_q_learning_noise(nl) for nl in noise_levels]\n",
        "dqn_noise = [simulate_performance_dqn_noise(nl) for nl in noise_levels]\n",
        "\n",
        "plot_comparison(noise_levels, q_learning_noise, dqn_noise, 'Performance vs Noise Level', 'Noise Standard Deviation')\n",
        "\n",
        "# 10. Training Time per Episode (This is tricky to simulate directly. We'll assume \"param\" here is episodes.)\n",
        "# We'll just treat param as episodes, since actual training time depends on hardware.\n",
        "# In a real scenario, you'd measure actual time. Here we just vary episodes.\n",
        "def simulate_performance_q_learning_time(ep):\n",
        "    return q_learning_allocation(subtasks, robots, episodes=int(ep))\n",
        "\n",
        "def simulate_performance_dqn_time(ep):\n",
        "    return dqn_allocation(subtasks, robots, episodes=int(ep), batch_size=32)\n",
        "\n",
        "training_times = np.linspace(0.1, 1.0, 10)\n",
        "q_learning_time = [simulate_performance_q_learning_time(int(tt*1000)) for tt in training_times]  # scale to episodes\n",
        "dqn_time = [simulate_performance_dqn_time(int(tt*1000)) for tt in training_times]\n",
        "\n",
        "plot_comparison(training_times, q_learning_time, dqn_time, 'Performance vs Training Time per Episode', 'Training Time (scaled as episodes)')\n"
      ],
      "metadata": {
        "id": "BKJnJDgxwc81",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ebd81c19-c275-466d-9a68-2ee249a4230c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/keras/src/layers/core/dense.py:87: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
            "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "J7Ibt06oGpfX"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}